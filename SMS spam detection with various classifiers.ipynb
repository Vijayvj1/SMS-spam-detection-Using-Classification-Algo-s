{"cells":[{"metadata":{"_uuid":"5992fc39-5db5-48d8-8514-bfdfd331e342","_cell_guid":"944986df-3828-4810-8443-cf0198a4ffca","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n%matplotlib inline\n\n\nsms = pd.read_csv('../input/spam.csv', encoding='latin-1')\nsms.head()\n\n\nsms = sms.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\nsms = sms.rename(columns = {'v1':'label','v2':'message'})\n\n\nsms.groupby('label').describe()\n\n\nsms['length'] = sms['message'].apply(len)\nsms.head()\n\n\nmpl.rcParams['patch.force_edgecolor'] = True\nplt.style.use('seaborn-bright')\nsms.hist(column='length', by='label', bins=50,figsize=(11,5))\n\n\ntext_feat = sms['message'].copy()\n\n\ndef text_process(text):\n    \n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    \n    return \" \".join(text)\n\n\ntext_feat = text_feat.apply(text_process)\n\n\nvectorizer = TfidfVectorizer(\"english\")\n\n\nfeatures = vectorizer.fit_transform(text_feat)\n\n\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3, random_state=111)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nsvc = SVC(kernel='sigmoid', gamma=1.0)\nknc = KNeighborsClassifier(n_neighbors=49)\nmnb = MultinomialNB(alpha=0.2)\ndtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\nlrc = LogisticRegression(solver='liblinear', penalty='l1')\nrfc = RandomForestClassifier(n_estimators=31, random_state=111)\nabc = AdaBoostClassifier(n_estimators=62, random_state=111)\nbc = BaggingClassifier(n_estimators=9, random_state=111)\netc = ExtraTreesClassifier(n_estimators=9, random_state=111)\n\n\n\nclfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc, 'AdaBoost': abc, 'BgC': bc, 'ETC': etc}\n\n\ndef train_classifier(clf, feature_train, labels_train):    \n    clf.fit(feature_train, labels_train)\n\n\ndef predict_labels(clf, features):\n    return (clf.predict(features))\n\n\npred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))\n\n\ndf = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score'])\ndf\n\n\ndf.plot(kind='bar', ylim=(0.9,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n\ndef stemmer (text):\n    text = text.split()\n    words = \"\"\n    for i in text:\n            stemmer = SnowballStemmer(\"english\")\n            words += (stemmer.stem(i))+\" \"\n    return words\n\n\ntext_feat = text_feat.apply(stemmer)\n\nfeatures = vectorizer.fit_transform(text_feat)\n\n\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3, random_state=111)\n\n\npred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))\n\n\ndf2 = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score2'])\ndf = pd.concat([df,df2],axis=1)\ndf\n\n\ndf.plot(kind='bar', ylim=(0.85,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n\n\nlf = sms['length'].as_matrix()\nnewfeat = np.hstack((features.todense(),lf[:, None]))\n\n\nfeatures_train, features_test, labels_train, labels_test = train_test_split(newfeat, sms['label'], test_size=0.3, random_state=111)\n\n\npred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))\n\n\ndf3 = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score3'])\ndf = pd.concat([df,df3],axis=1)\ndf\n\n\ndf.plot(kind='bar', ylim=(0.85,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n\nfrom sklearn.ensemble import VotingClassifier\neclf = VotingClassifier(estimators=[('BgC', bc), ('ETC', etc), ('RF', rfc), ('Ada', abc)], voting='soft')\neclf.fit(features_train,labels_train)\npred = eclf.predict(features_test)\n\n\nprint(accuracy_score(labels_test,pred))\n\n\n","execution_count":0,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":4}